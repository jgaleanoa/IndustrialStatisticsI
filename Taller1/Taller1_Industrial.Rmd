---
header-includes:
- \usepackage{longtable}
- \usepackage[utf8]{inputenc}
- \usepackage[spanish]{babel}\decimalpoint
- \setlength{\parindent}{1.25cm}
- \usepackage{amsmath}
- \usepackage{xcolor}
- \usepackage{cancel}
- \usepackage{array}
- \usepackage{float}
- \usepackage{multirow}
output:
  pdf_document:
    number_sections: true
fontsize: 12pt
papersize: letter
geometry: margin = 1in
language: "es"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, fig.align = "center")
library(kableExtra)
library(knitr)
library(tidyverse)
library(qcc)
library(latex2exp)
```

\input{titlepage}
\thispagestyle{empty}
\tableofcontents

\newpage

\pagestyle{myheadings}
\setcounter{page}{3}

<!-- Juanjo ------------------------------------------------------ -->

\section{Ejercicio 1}

El peso neto (en onzas) de un producto blanqueador en polvo va a
monitorearse con cartas de control $\bar{x}$ y $R$ utilizando un tamaño de
la muestra de $n = 4$. Se registran datos de 25 muestras.

```{r datos1}
datos1 <- read.table("DatosEJ1.txt", header = T) %>% 
  select(-Numero_de_muestra)
```

\subsection{Prueba de bondad de ajuste a los datos}

La metodología considerada funciona bajo el supuesto de que los datos
provienen de una distribución normal, por lo que es muy importante chequear
dicho supuesto antes de proceder con cálculos. 

```{r hist-ej1, fig.cap="Gráficos para chequeo de normalidad"}
full_sample <- datos1 %>% 
  as.matrix() %>% 
  as.vector() 

qqplot <- ggplot(mapping = aes(sample = full_sample)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Cuantiles teóricos", 
       y = "Cuantiles muestrales",
       title = "Gráfico de probabilidad normal") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

hist_peso <- ggplot(mapping = aes(x = full_sample)) +
  geom_histogram(bins = nclass.Sturges(full_sample), 
                 col = "black", fill = "lightgreen") +
  labs(x = "Peso neto (oz)", 
       y = "Frecuencia", 
       title = "Distribución del peso neto(oz)") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

ggpubr::ggarrange(qqplot, hist_peso, nrow = 1, ncol = 2)
```

En el gráfico de probabilidad normal se aprecia que hay múltiples unidades
experimentales que tienen el mismo peso. Aunque los puntos no se desvían
mucho de la recta, dicho supuesto se ve defectuoso en el histograma puesto
que se observa asimetría negativa.

Además de los análisis gráficos, se usa la prueba de normalidad de Shapiro
- Wilk. \newline Sea $X:$ Peso neto de un producto blanqueador en onzas.

\newpage

Se quiere probar

$$
\begin{cases}
H_0: X \sim N(\mu, \sigma^2) \\
H_1: X \nsim N(\mu, \sigma^2) 
\end{cases}
$$

Luego de realizar la prueba de normalidad se obtienen los siguientes
resultados

```{r shapiro-wilk1}
test_norm <- data.frame(W = 0.9665, pvalue = 0.01201)
kable(test_norm, longtable = T, booktabs = T, 
      caption = "Prueba de normalidad", 
      col.names = c("W", "Valor-p"))
```

El valor-p es polémico a un nivel de significancia $\alpha = 0.01$ debido a
que se encuentra muy cerca del límite de decisión. Si bien dicho valor es
mayor a 0.01, no rechazar $H_0$ es una decisión poco fiable.

Vistos los análisis previos, se considera que la distribución normal no se
ajusta bien al peso neto del producto blanqueador, sin embargo para fines 
de ilustrar la metodología de control del proceso se asume que las
desviaciones del supuesto no son muy graves.

\subsection{Gráficos de control para $\bar{x}$ y $R$}

Inicialmente se debe verificar el gráfico de Shewhart para R puesto que los
límites de control para $\bar{x}$ dependen de $R$, luego es necesario
asegurarse que la variabilidad de $R$ este bajo control antes de proceder
con $\bar{x}$.

$$
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i \hspace{.2in} \bar{\bar{x}} = \frac{1}{m} \sum_{i=1}^{m} \bar{x}_i \hspace{.2in} \bar{R} = \frac{1}{m} \sum_{i=1}^m R_i
$$

\begin{table}[H]
\centering
\caption{Límites de control para $\bar{x}$ y $R$}
\begin{tabular}{|l|l|l|l|}
\hline
   & LCL & CL & UCL \\ \hline
$\bar{x}$  & $\bar{\bar{x}} - A_2 \bar{R}$   & $\bar{\bar{x}}$  & $\bar{\bar{x}} + A_2 \bar{R}$   \\ \hline
$R$ & $\bar{R}D_3$   & $\bar{R}$  & $\bar{R}D_4$ \\ \hline  
\end{tabular}
\end{table}

```{r control-R, fig.height=6, fig.width=6}
control_r1 <- qcc(datos1, type = "R", title= "Gráfico de Shewhart para R",
                  xlab = "Muestra", ylab = "Rango")

```

Puesto que ningún punto cae fuera de los límites de control ni tampoco se
observan patrones sistemáticos en el gráfico, se puede proceder a la
construcción del diagrama de Shewhart para $\bar{x}$.

```{r control-xbar, fig.height=6, fig.width=6}
control_xbar1 <- qcc(datos1, type = "xbar", 
                     title = "Gráfico de Shewhart para el peso promedio",
                     xlab = "Muestra", ylab = "Peso promedio (oz)")
```

Similar al caso de $R$, todos los puntos caen dentro de los límites de
control y no se observan patrones sistemáticos, se puede concluir que el
proceso está bajo control. 

Para finalizar, se verifica el valor de los límites de control obtenidos en
R con el paquete `qcc` luego de hacer los calculos manuales.

```{r calculos-manuales}
x_doble_bar <- apply(datos1, 1, mean) %>% 
  mean()
R_bar <- apply(datos1, 1, function(x) max(x) - min(x)) %>% 
  mean()
```


$$
\bar{\bar{x}} = \frac{1}{25} \sum_{i=1}^{25} \bar{x}_i = `r x_doble_bar` \hspace{.2in} \bar{R} = \frac{1}{25} \sum_{i=1}^{25} R_i = 0.464
$$

**Para $\bar{x}$**

Se tiene que $n = 4$, luego $A_2 = 0.729$ por lo tanto los límites de
control son

$$
\begin{aligned}
\text{UCL} &= `r x_doble_bar` + 0.729 \times `r R_bar`=`r x_doble_bar + 0.729*R_bar` \\
\text{LCL} &= `r x_doble_bar` - 0.729 \times `r R_bar`=`r x_doble_bar - 0.729*R_bar`
\end{aligned}
$$
estos límites concuerdan con los obtenidos en R a tres cifras decimales, sin
embargo hay que tener en cuenta que los calculados por el software son más
precisos debido a que consideran más números decimales.

**Para R**

Nuevamente, debido a $n = 4$ se tiene $D_3 = 0$ y $D_4 = 2.282$, así

$$
\begin{aligned}
\text{UCL} &= `r R_bar` \times 2.282 = `r R_bar*2.282` \\
\text{LCL} &= `r R_bar` \times 0 = 0
\end{aligned}
$$
los cuales a cuatro cifras decimales también concuerdan con los obtenidos 
en el lenguaje de programación R.

\subsection{Estimación de media y varianza del proceso}

Una vez verificado el control del proceso, resulta de interés estimar los
parámetros $\mu$ y $\sigma$ de la distribución normal para realizar
cálculos como lo puede ser, el porcentaje de producto blanqueador cuyo 
peso neto este fuera de los límites de control.

El estimador de $\mu$ es $\bar{\bar{x}} = 16.239$ el cual fue calculado en
el inciso anterior. Por otro lado, para la desviación estándar se tiene
$\hat{\sigma} = \frac{\bar{R}}{d_2}$; en el inciso anterior se llegó a
$\bar{R} = `r R_bar`$ y además para $n = 4$ se tiene $d_2 = 2.059$
obteniendo así $\hat{\sigma} = \frac{0.464}{2.059} = `r 0.464/2.059`$.

En resumen

$$
\hat{\mu} = 16.239 \hspace{.35in} \hat{\sigma} = `r 0.464/2.059`
$$

\subsection{Porcentaje de defectuosos}

Suponiendo que el fabricante establece como medidas de especificación para
el peso del producto blanqueador $16 \pm 0.75 \text{ oz}$ el porcentaje de
defectuosos se puede calcular como 
$\left[1 - \mathbb{P}(15.25\leq X \leq 16.75)\right] \times 100\%$
con $X \sim N\left(16.239, \ 0.2253521^2\right)$.

```{r porc-def}
prob_in <- pnorm(16.75, 16.239, 0.2253521) - pnorm(15.25, 16.239, 0.2253521)
```

Se puede verificar que $\mathbb{P}(15.25\leq X \leq 16.75) = `r prob_in`$
por lo que el porcentaje de productos defectuosos es 
$(1 - `r prob_in`) \times 100\% = `r (1 - prob_in)*100` \%$, es decir, la
cantidad de producto blanqueador producido cuyo peso será menor a 15.25 o
mayor a 16.75 oz es aproximadamente el 1.1684 \%.

\subsection{¿Cuándo se espera perder el control?}

\subsection{Desplazamiento de media}

Se define $\beta = \mathbb{P}(\text{No detectar el desplazamiento de media | La media se desplazo})$. 
Si la media del proceso se desplaza a $\mu_1 = 16.2$ y la desviación
estándar permanece constante, entonces:

$$
\bar{x} \sim N\left(16.2, \frac{0.2253521^2}{4}\right)
$$

```{r beta}
sigma1 <- R_bar/2.059
beta1 <- pnorm(16.577256, 16.2, sigma1/2) - pnorm(15.900744, 16.2, sigma1/2)
first_sample <- beta1^(1 - 1)*(1 - beta1)
```


$$
\begin{aligned}
\beta &= \mathbb{P}(15.900744 \leq \bar{x} \leq 16.577256 \ | \ \hat{\mu} = 16.2, \ \hat{\sigma} = 0.2253521) \\
&= `r beta1`
\end{aligned} 
$$
Sea $Y: \text{Número de muestras revisadas hasta detectar cambio en la media del proceso}$

$$
\begin{aligned}
&Y \sim Geo(1 - \beta) \Leftrightarrow p_Y(y) = \beta^{y-1}(1 - \beta) \ \text{con} \ \beta = `r beta1` \\
&\mathbb{P}(Y = 1) = `r 1-beta1`
\end{aligned}
$$

El cual es un resultado razonable teniendo en cuenta que el desplazamiento
de la media fue pequeño numéricamente hablando (0.039 oz).

\subsection{Muestras necesarias para detectar cambios}

Para este caso se quiere calcular el $ARL_1$ (Average run length) puesto que
se esta suponiendo un proceso fuera de control por una diferencia de 0.039
respecto al promedio del proceso cuando está en control.

$ARL_1 = \frac{1}{1-\beta} = \frac{1}{1-`r beta1`} = `r 1/(1 - beta1)`$, es
decir, el número esperado de muestras a revisar hasta encontrar un cambio en
el peso del producto blanqueador de 0.039 oz es aproximadamente 229.27
muestras.

\subsection{Curva de operación característica}

Para finalizar, se gráfica la curva de operación característica (OC), la
cual muestra como disminuye $\beta$ a medida que el desplazamiento de la
media aumentaba para diferentes valores de $n.$

<!-- Gaviria ----------------------------------------------------- -->

\section{Ejercicio 2}


<!-- Juanjo ------------------------------------------------------ -->

\section{Ejercicio 5}

<!-- Simon ------------------------------------------------------- -->

\section{Ejercicio 6}